{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A convolution filter, often simply called a convolution, is a fundamental operation in convolutional neural networks (CNNs) and image processing. Its primary function is to detect and extract features from input data, which can be images, audio, or other types of data.\n",
    "\n",
    "\n",
    "Types of convolution filter: \n",
    "1. averaging filter \n",
    "2. guassian blurring -> reduces image noise\n",
    "3. edge detection (like the sobel filter but there are other filters with different values) -> Filters can be used as a fetaure extractor -> edges are a type of feature\n",
    "\n",
    "What is a convolution filter ? \n",
    "A convolution filter, often simply called a convolution, is a fundamental operation in convolutional neural networks (CNNs) and image processing. Its primary function is to detect and extract features from input data, which can be images, audio, or other types of data.\n",
    "\n",
    "Output of a convolution is not an image, its a ***feature map***\n",
    "\n",
    "If I have a problem and I dont know which filter I should use to extract these features, I initialize a filter with randomly initalized values and pass the image to it and then pass this feature map top anpther random vonvolution for N times, then take the final op and flatten it (yhat). Calculate loss between yhat and y and the NN will update the filter values as it back probagates. \n",
    "\n",
    "Hyperparameters we would like to control: step of filter/??????\n",
    "\n",
    "CNN'S are used for image classification , objec detection , image segmentation \n",
    "\n",
    "Convolutional Neural Networks (CNNs) are extensively used in transfer learning. The pre-trained CNN models, which have been trained on large datasets like ImageNet, can be fine-tuned to perform specific tasks on new, often smaller, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per single stride, each frame B , G , and R -> multipleade by the filter result in V1, V2, and V3. These three elements are added together plus bias 'B'. This final sum is the first element of the op feature map. This happens until the kernel went over the whole image forming the whole feature map. This feature map size could either be equal to the image (if padding was used , stride , filer size ) or could be smaller (if padding aint used,  stride , filer size). \n",
    "This feature map aize MxM, is then input to the actovation functions which results in another feature map also size MxM with 0's in the pixels where the pattern wasnt found. This informs the next layer if the previous layer found th e pattern or not. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are keypoints in an image but there are a lot of dummy data as well. Key points distinctive locations within an image that are used for tasks such as image matching, object recognition, and tracking. Keypoints are crucial because they represent important features of an image that can be reliably identified and used to describe the image in a compact form.\n",
    "\n",
    "We are able to detect/extract these keypoints from the input\n",
    "\n",
    "Application of key points: \n",
    "\n",
    "- Image Matching and Registration: Keypoints are used to find correspondences between different images of the same scene or object. This is useful in applications like panorama stitching and 3D reconstruction.\n",
    "\n",
    "- Object Recognition: By matching keypoints between a known object and an image, the object can be recognized and its position and orientation determined.\n",
    "\n",
    "- Tracking: Keypoints can be tracked across a sequence of video frames to follow the movement of objects.\n",
    "\n",
    "- Robotics: Keypoints are used in visual SLAM (Simultaneous Localization and Mapping) to help robots understand and navigate their environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling layer\n",
    "types: \n",
    "1. avg pooling \n",
    "2. max poolinmg \n",
    "\n",
    "Function -> decreases noise and complexity which also  decreases overfitting \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: \n",
    "**- Cnn's when used with videos, there's a 3rd demiension as ip which is the fps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: \n",
    "**- A Cnn can lack a fully connected layer, depoending on the NN's function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Convolutional Neural Networks (CNNs), several hyperparameters can significantly influence the performance, efficiency, and accuracy of the model. These hyperparameters control various aspects of the network architecture, the convolutional operations, and the training process. Here are some key hyperparameters you might want to control in a CNN:\n",
    "\n",
    "### Convolutional Layer Hyperparameters\n",
    "\n",
    "These parametrs are specified per a single convolution layer \n",
    "\n",
    "1. **Filter Size (Kernel Size)**\n",
    "   - **Description**: The dimensions of the filters (kernels) applied during the convolution operation, typically represented as (height, width).\n",
    "   - **Example**: Common filter sizes include 3x3, 5x5, and 7x7.\n",
    "\n",
    "2. **Number of Filters**\n",
    "   - **Description**: The number of filters in a convolutional layer, which determines the number of output feature maps.\n",
    "   - **Example**: A layer might have 32, 64, or 128 filters.\n",
    "\n",
    "3. **Stride**\n",
    "   - **Description**: The number of pixels by which the filter moves across the input image.\n",
    "   - **Example**: A stride of 1 means the filter moves one pixel at a time, while a stride of 2 means it moves two pixels at a time.\n",
    "\n",
    "4. **Padding**\n",
    "   - **Description**: The addition of extra pixels around the border of the input to control the spatial size of the output feature maps.\n",
    "   - **Types**: \n",
    "     - **Valid Padding** (no padding): Output size is smaller than the input size.\n",
    "     - **Same Padding**: Padding is added to keep the output size the same as the input size.\n",
    "5. **Activation**\n",
    " - **Description**: the addition of an activation function per layer will indicate if the filter found its pattern or not. If its relu, and it found the pattern it'll treutirn input, if not itll return 0. Relu = max(input,0)\n",
    "\n",
    "### Pooling Layer Hyperparameters\n",
    "\n",
    "1. **Pooling Type**\n",
    "   - **Description**: The type of pooling operation used to downsample the feature maps.\n",
    "   - **Types**: Max pooling, average pooling.\n",
    "\n",
    "2. **Pooling Size**\n",
    "   - **Description**: The dimensions of the pooling filter, typically represented as (height, width).\n",
    "   - **Example**: Common pooling sizes include 2x2 and 3x3.\n",
    "\n",
    "3. **Stride**\n",
    "   - **Description**: The number of pixels by which the pooling filter moves across the input.\n",
    "   - **Example**: Stride depends on the pooling size if pooling size == 2x2 , stride is 2 if its 3x3 , stride is 3\n",
    "   \n",
    " \n",
    "# Hyperermarameters in other topics\n",
    "\n",
    "### Fully Connected Layer Hyperparameters\n",
    "\n",
    "1. **Number of Neurons**\n",
    "   - **Description**: The number of neurons in each fully connected layer.\n",
    "   - **Example**: A fully connected layer might have 128, 256, or 512 neurons.\n",
    "\n",
    "### Training Hyperparameters\n",
    "\n",
    "1. **Learning Rate**\n",
    "   - **Description**: The step size used by the optimization algorithm to update the weights.\n",
    "   - **Example**: Typical values range from 0.001 to 0.1.\n",
    "\n",
    "2. **Batch Size**\n",
    "   - **Description**: The number of training examples used in one forward/backward pass.\n",
    "   - **Example**: Common batch sizes include 32, 64, and 128.\n",
    "\n",
    "3. **Number of Epochs**\n",
    "   - **Description**: The number of times the entire training dataset is passed through the network.\n",
    "   - **Example**: A typical number of epochs might be 10, 50, or 100.\n",
    "\n",
    "4. **Momentum**\n",
    "   - **Description**: A parameter that helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n",
    "   - **Example**: Common values range from 0.9 to 0.99.\n",
    "\n",
    "5. **Regularization Parameter (e.g., L2 Regularization)**\n",
    "   - **Description**: A parameter used to prevent overfitting by penalizing large weights.\n",
    "   - **Example**: A common value for L2 regularization is 0.001.\n",
    "\n",
    "6. **Dropout Rate**\n",
    "   - **Description**: The fraction of input units to drop during training to prevent overfitting.\n",
    "   - **Example**: Typical dropout rates are 0.25, 0.5.\n",
    "\n",
    "### Optimization Algorithm Hyperparameters\n",
    "\n",
    "1. **Type of Optimizer**\n",
    "   - **Description**: The optimization algorithm used to minimize the loss function.\n",
    "   - **Examples**: Stochastic Gradient Descent (SGD), Adam, RMSprop.\n",
    "\n",
    "### Network Architecture Hyperparameters\n",
    "\n",
    "1. **Number of Layers**\n",
    "   - **Description**: The total number of layers in the network, including convolutional, pooling, and fully connected layers.\n",
    "   - **Example**: A network might have 5, 10, or more layers.\n",
    "\n",
    "2. **Activation Functions**\n",
    "   - **Description**: The activation function used in each layer to introduce non-linearity.\n",
    "   - **Examples**: ReLU, Sigmoid, Tanh.\n",
    "\n",
    "### Example Configuration\n",
    "\n",
    "Here’s an example configuration of hyperparameters for a CNN used for image classification:\n",
    "\n",
    "- **Convolutional Layers**:\n",
    "  - Layer 1: 32 filters, 3x3 kernel, stride 1, same padding\n",
    "  - Layer 2: 64 filters, 3x3 kernel, stride 1, same padding\n",
    "\n",
    "- **Pooling Layers**:\n",
    "  - Layer 1: Max pooling, 2x2 pooling size, stride 2\n",
    "\n",
    "- **Fully Connected Layers**:\n",
    "  - Layer 1: 128 neurons, ReLU activation\n",
    "\n",
    "- **Training**:\n",
    "  - Learning rate: 0.001\n",
    "  - Batch size: 64\n",
    "  - Number of epochs: 20\n",
    "  - Optimizer: Adam\n",
    "\n",
    "By carefully tuning these hyperparameters, you can optimize the performance of your CNN for the specific task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings \n",
    "filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 1412s 8us/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense,Activation, Dropout,Flatten,Conv2D,MaxPooling2D, BatchNormalization, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_train,y_train),(x_test,y_test)= cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24500, 32, 32, 3)\n",
      "(10500, 32, 32, 3)\n",
      "(24500, 1)\n",
      "(10500, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.3)\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_val = to_categorical(y_val, num_classes=10)\n",
    "y_test = to_categorical(y_test , num_classes = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installatioin\n",
    "AlexNet=Sequential()\n",
    "\n",
    "#1st Convolution Layer\n",
    "AlexNet.add(Conv2D(filters=96,input_shape=(32,32,3),kernel_size=(11,11),strides=(4,4),padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#2nd Maxpooling Convolution Layer\n",
    "AlexNet.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\n",
    "\n",
    "#3rd Convolution Layer\n",
    "AlexNet.add(Conv2D(filters=256,kernel_size=(5,5),strides=(1,1),padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#4 Maxpooling Convolution Layer\n",
    "AlexNet.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\n",
    "\n",
    "#5 Convolution Layer\n",
    "AlexNet.add(Conv2D(filters=384,kernel_size=(3,3),strides=(1,1),padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#6 Convolution Layer\n",
    "AlexNet.add(Conv2D(filters=384,kernel_size=(3,3),strides=(1,1),padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#7 Convolution Layer\n",
    "AlexNet.add(Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "\n",
    "#8 Maxpooling Convolution Layer\n",
    "AlexNet.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\n",
    "\n",
    "#9 FC layer \n",
    "AlexNet.add(Flatten())\n",
    "AlexNet.add(Dense(4096  , activation = 'relu'))\n",
    "\n",
    "#10 FC layer \n",
    "AlexNet.add(Dense(4096  , activation = 'relu'))\n",
    "\n",
    "#Op layer\n",
    "AlexNet.add(Dense(10  , activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model with an optimizer and a loss function \n",
    "from keras.optimizers import Adam\n",
    "\n",
    "AlexNet.compile(optimizer = Adam() , loss = 'categorical_crossentropy', metrics = ['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 465s 606ms/step - loss: 1.9516 - accuracy: 0.2518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e7e77f9e50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model \n",
    "AlexNet.fit(x_train , y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is very bad accuracy^ check this out and fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "245/245 [==============================] - 370s 2s/step - loss: 1.5906 - accuracy: 0.3816 - val_loss: 2.4379 - val_accuracy: 0.2368\n",
      "Epoch 2/5\n",
      "245/245 [==============================] - 539s 2s/step - loss: 1.4621 - accuracy: 0.4461 - val_loss: 1.5416 - val_accuracy: 0.4294\n",
      "Epoch 3/5\n",
      "245/245 [==============================] - 416s 2s/step - loss: 1.3496 - accuracy: 0.5016 - val_loss: 1.4862 - val_accuracy: 0.4661\n",
      "Epoch 4/5\n",
      "245/245 [==============================] - 6124s 25s/step - loss: 1.2744 - accuracy: 0.5342 - val_loss: 1.3023 - val_accuracy: 0.5332\n",
      "Epoch 5/5\n",
      "245/245 [==============================] - 406s 2s/step - loss: 1.2006 - accuracy: 0.5713 - val_loss: 1.4809 - val_accuracy: 0.4674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e7e7c89a90>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_generator=ImageDataGenerator(rotation_range=2,horizontal_flip=True,zoom_range=.1)\n",
    "val_generator=ImageDataGenerator(rotation_range=2,horizontal_flip=True,zoom_range=.1)\n",
    "test_generator=ImageDataGenerator(rotation_range=2,horizontal_flip=True,zoom_range=.1)\n",
    "\n",
    "train_generator.fit(x_train)\n",
    "val_generator.fit(x_val)\n",
    "test_generator.fit(x_test)\n",
    "\n",
    "x_train_generator=train_generator.flow(x_train,y_train,batch_size=100)\n",
    "x_val_generator=val_generator.flow(x_val,y_val,batch_size=100)\n",
    "x_test_generator=test_generator.flow(x_test,y_test,batch_size=100)\n",
    "\n",
    "AlexNet.fit(\n",
    "    x_train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=x_val_generator,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Get the list of all physical devices\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Count the number of GPUs\n",
    "num_gpus = len(physical_devices)\n",
    "\n",
    "print(\"Number of GPUs:\", num_gpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atalla3 el predictions \n",
    "#evaluate model using confusion matrix / classification report , f1 score f2 score , percision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new task \n",
    "\n",
    "1-IMPLEMENT INCEPTION NN NAMED GOOGLENET \n",
    "2- DIFFERENT APPLICATION ON SIFT ALGO (SLAM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
